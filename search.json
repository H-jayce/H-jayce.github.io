[{"title":"loss_function","date":"2021-08-20T16:00:00.000Z","url":"/2021/08/21/Loss_function/","tags":[["图神经网络","/tags/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"]],"categories":[["机器学习","/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"]],"content":"来自：一文看尽深度学习中的15种损失函数 损失函数 [loss_function: ] 用于定义单个训练样本与真实值之间的误差；用于衡量模型所作出的预测与真实值（Ground Truth）之间的偏离程度。 回归损失（针对连续型变量）；连续函数 分类损失（针对离散型变量）。分段函数 1. 回归损失（Regression Loss）+++不好用： 1.1 L1 Loss也称为Mean Absolute Error，即平均绝对误差（MAE），它衡量的是预测值与真实值之间距离的平均误差幅度，作用范围为0到正无穷 1.2 L2 Loss也称为Mean Squred Error，即均方差（MSE），它衡量的是预测值与真实1值之间距离的平方和，作用范围为0到正无穷。+++ 1.3 Smooth L1 Loss平滑的L1损失（SLL），出自Fast RCNN 。SLL通过综合L1和L2损失的优点，在0点处附近采用了L2损失中的平方函数，解决了L1损失在0点处梯度不可导的问题，使其更加平滑易于收敛。此外，在|x|&gt;1的区间上，它又采用了L1损失中的线性函数，使得梯度能够快速下降。 SLL(x) = \\begin{cases} 0.5 x^2 \\qquad if\\ |x|"},{"title":"deep_learning","date":"2021-08-20T16:00:00.000Z","url":"/2021/08/21/deep_learning/","categories":[["机器学习","/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"]],"content":"深度学习《深度学习》- [美]伊恩-古德费洛 《动手学深度学习》 $\\vec{a}$ 向量$\\overline{a}$ 平均值$\\widehat{a}$ (线性回归，直线方程) y尖$\\widetilde{a}$ 颚化符号 等价无穷小$\\dot{a}$ 一阶导数$\\ddot{a}$ 二阶导数 $\\cfrac 1N $ 分数 准确率accuracy : 做出正确预测的数量与预测的数量之比。 accuracy(\\widehat{y},y)= \\begin{cases} 1, &if\\ \\widehat{y}_i=y_i \\\\ 0, &if\\ 其他 \\end{cases} 对于包含N个元素的完整数据集，取所有样本的平均精度： accuracy(D)=\\cfrac 1N \\sum_{i=1}^N accuracy(\\widehat{y}_i,y_i) 分类：使用混淆矩阵 平均绝对误差MAE：原始值和预测值之间的绝对差的平均值。 MAE=\\cfrac 1N \\sum^N_{i=1} |y_i-\\widehat{y}_i|没有上限，下限为0,希望其值接近0 。 均方误差MSE ：原始值与预测值之差的平方的平均值。 MSE = \\cfrac 1N \\sum^N_{i=1}(y_i-\\widehat{y}_i)^2 ++++ 防止过拟合数据：正则化技术（对参数的L2惩罚、dropout、批量归一化、数据扩充等。） +++++ 损失函数：定义模型输出和实际数据之间的的关系 L = \\cfrac 1k \\sum^k_{i=1} ||\\widehat{y}_i-y_i||_2损失函数的最小化是通过对模型的参数值进行小的迭代调整来实现的。 +++ 优化： 梯度下降法 梯度下降优化算法 反向传播和自动微分 +++ 卷积神经网络CNN是现代计算机视觉、语音识别甚至自然语言处理应用的基本构件。 很容易地扩展到具有很大宽度和高度的图像。 卷积核(kernel) ; 输入尺寸(input) ; 步长(stride) ; 填充(padding) 输出尺寸=(输入尺寸-filter尺寸+2*padding)/stride+1宽和高都是这么计算的； 输入图片大小为200×200，依次经过一层卷积（kernel size 5×5，padding 1，stride 2），pooling（kernel size 3×3，padding 0，stride 1），又一层卷积（kernel size 3×3，padding 1，stride 1）之后，输出特征图大小为：97 计算尺寸不被整除只在GoogLeNet中遇到过。卷积向下取整，池化向上取整。（200-5+21）/2+1 为99.5，取99（99-3）/1+1 为97（97-3+21）/1+1 为97 常见的：stride为1,kernel为 3 ,padding为1 卷积前后尺寸不变stride为1,kernel为 5 ,padding为2 卷积前后尺寸不变 +++++ 循环神经网络RNN可以扩展到更长的序列，是一类用于处理序列数据的神经网络。 长短期记忆LSTM LSTMs的输入必须是三维的（三维的结构是[样本批大小，滑窗大小，特征数量] 输出为[批量大小，滑窗大小] BaseRNN_Cellstate_size(隐藏层的大小)，output_size(输出的大小)，输出大小一般等于最后一层RNN的state_size. 如：输入数据(batch_size, input_size),那么计算时得到的隐藏层状态就是(batch_size, state_size),输出层就是(batch_size, output_size) = (batch_size, 最后一层state_size). 时间维度静态展开：Static_RNN时间维度动态展开：dynamic——rnn输入数据的格式为(batch_size, time_steps, input_size) batch_size: 表示batch的大小，即一个batch中序列的个数; time_steps: 表示序列本身的长度，长度为10的句子对应的time_steps就等于10; input_size: 表示输入数据单个序列单个时间维度上固有的长度。 +++ Keras中主要的模型是Sequential模型，Sequential模型是一系列网络层按顺序构成的栈。 from keras.model import sequential model = sequential() # 将一些网络层通过.add()堆叠起来，就构成了一个模型： from keras.layers import Dense, Activation model.add(Dense(output_dim=64, input_dim=100)) model.add(Activation(&quot;relu&quot;)) model.add(Dense(output_dim=10)) model.add(Activation(&quot;softmax&quot;)) # 完成模型的搭建后，需要使用compile()方法来编辑模型： model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&#39;sgd&#39;, metrics=[&#39;accuracy&#39;]) # 完成模型编译后，我们在训练数据上按batch进行一定次数的迭代训练，以拟合网络： model.fix(x_train, y_train, nb_epoch=5, batch_size=32) loss_and_metrics = model.evaluate(x_test, y_test, batch_size=32) # 评估 pre = model.predict_proba(x_test, batch_size=32) # 预测 Flatten层，用来将输入“压平”，即把多维的输入一维化，常用在从卷积层到全连接层的过渡。 Embedding层，嵌入层， +++ 绘制曲线acc = history.history[&#39;acc&#39;] val_acc = history.history[&#39;val_acc&#39;] loss = history.history[&#39;loss&#39;] val_loss = history.history[&#39;val_loss&#39;] epochs = range(1, len(acc)+1) plt.plot(epochs,acc,&#39;bo&#39;,label=&#39;Training acc&#39;) plt.plot(epochs,val_acc,&#39;b&#39;,label=&#39;Validation acc&#39;) plt.title(&#39;Training and validation accuracy&#39;) plt.legend() plt.figure() plt.plot(epochs,loss,&#39;bo&#39;,label=&#39;Training loss&#39;) plt.plot(epochs,val_loss,&#39;b&#39;,label=&#39;Validation loss&#39;) plt.title(&#39;Training and validation loss&#39;) plt.legend()"},{"title":"开发笔记_1","date":"2021-08-17T16:00:00.000Z","url":"/2021/08/18/Genvi_KaiFa/","tags":[["Genvi开发","/tags/Genvi%E5%BC%80%E5%8F%91/"]],"categories":[["编程","/categories/%E7%BC%96%E7%A8%8B/"]],"content":"1. 工程新建1.1 新建DLL 工程文件 file —&gt;new —&gt;project; Visual c++ —&gt;win32 —&gt;win32 project; next —&gt;dll —&gt;MFC。 1.2 添加头文件内容 复制原“stdafx.h“ 内容(就这点东西，艹了，直接挂掉)。 (#include 头文件路径自己调整) “iMath.h”(不会用，不知道计算原理，使用很危险) 原文本编码格式“GB18030” 1.3 工程环境依赖 Properties—&gt;General—&gt;Target Platform Version—&gt;10.0.14393.0；(软件开发平台工具包) Properties—&gt;General—&gt;Use of MFC—&gt;Use MFC in a Shared DLL；(微软cpp封装API，界面开发) Properties—&gt;Debugging—&gt;Command—&gt;$(OutDir)\\Genvi.exe；Release + x64;(调试路径)复制—&gt;之前$(OutDir)路径下的文件一股脑all in；(不认识的东西，应该是调试的依赖) Properties—&gt;C/C++—&gt;General—&gt;Additional Include Directories—`$(SolutionDir)Gdemrt；(头文件)复制—&gt;$(SolutionDir)Gdemrt； Properties—&gt;Linker—&gt;General—&gt;Additional Library Direction—$(OutDir)Properties—&gt;Linker—&gt;Input—&gt;Additional Dependencies—&gt;Gdemrt.lib.(链接库)复制—&gt;$(OutDir)Gdemrt.lib "},{"title":"Graph Neural Network","date":"2021-08-17T16:00:00.000Z","url":"/2021/08/18/Graph_Neural_Network/","tags":[["图神经网络","/tags/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"]],"categories":[["机器学习","/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"]],"content":"图神经网络"},{"title":"Pytorch_GPU","date":"2021-08-16T16:00:00.000Z","url":"/2021/08/17/Pytorch_GPU/","tags":[["软件","/tags/%E8%BD%AF%E4%BB%B6/"]],"categories":[["机器学习","/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"]],"content":"PyTorch是一个开源的Python机器学习库，基于Torch，用于自然语言处理等应用程序。 2017年1月，由Facebook人工智能研究院（FAIR）基于Torch推出了PyTorch。它是一个基于Python的可续计算包，提供两个高级功能： 1、具有强大的GPU加速的张量计算（如NumPy）。 2、包含自动求导系统的深度神经网络。 1. 安装conda1.1 Anaconda安装包1.2 安装annacondabash Anaconda*.sh 1.3 检查conda版本conda --version 2. 安装NVIDIA2.1 sudo apt install nvidia-driver-4402.2 输入secure boot 密码2.3 关机重启，进入界面2.4 选择enroll mok $\\rightarrow$ continue $\\rightarrow$ yes $\\rightarrow$输入设置的secure boot密码 $\\rightarrow$​ reboot2.5 进入桌面nvidia-smi 3.安装CUDA3.1 sudo apt install nvidia-cuda-toolkit3.2 查看版本nvcc -V 4. 安装PyTorch_GPUPyTorch官网 4.1 conda添加清华源conda config --add channels  conda config --add channels  conda config --add channels  conda config --set show_channel_urls yes 4.2 CPUconda install pytorch torchvision torchaudio cpuonly -c pytorch 4.3 GPUconda install pytorch torchvision torchaudio cudatoolkit=10.1 5. 检查5.1 PyTor_CPUpython import torch print(torch.rand(5, 3)) 1 2 3 4 5tensor([[0.7963, 0.8564, 0.6089], [0.0640, 0.8827, 0.4916], [0.9320, 0.1722, 0.3863], [0.3529, 0.3285, 0.3255], [0.1337, 0.7180, 0.5241]]) 复制 5.2 PyTor_GPUpython import torch torch.cuda.is_available() 1 True "},{"title":"cpp","date":"2021-08-16T16:00:00.000Z","url":"/2021/08/17/cpp/","tags":[["学习","/tags/%E5%AD%A6%E4%B9%A0/"]],"categories":[["学习","/categories/%E5%AD%A6%E4%B9%A0/"]],"content":"1.Python格式化字符 More info: 格式化输出 2.markdown中公式编辑教程 编程——计算——建模——抽象——设定范围。 编程步骤：输入——处理——输出。 3.1 基础如何编译，调试，环境设置; C++语言，语法，语义基础。 3.2 核心面向对象思想;封装，继承，多态;模板，STL;IO，Lambda。 4.1 教材：C++编辑思想、C++程序设计语言、软件调试的艺术。4.2 VirtualBox导入镜像文件 创建文本：touch test.cpp 编译链接：g++ test.cpp 显示当前目录全部内容：ls -lart,直接使用 ll C++标识符C++是区分大小写的编程语言 C/C++基本编译和执行过程预处理——头文件（.hpp）和宏文件展开——生成(.i)文件编译——词法分析，优化，代码生成——（.asm）文件，汇编代码——生成(.o/.obj)文件链接——将（.o）文件和动态库文件（.so）链接——生成可执行文件执行可执行文件预处理——cpp test.cpp&gt;test.i 编译——g++ -g -S test.i 汇编—— as -o test.o test.s gcc -g -c test.c; objdump -d -M intel -S test.o 链接——Id -o myTest test.o …[其他链接选项] C/C++核心调试技术：GDB4.1 直接调试：gdb a.out break main run ​ 4.2 通过界面调试 gdb test --tui next 对象，类型，变量和值对象 类型 变量 值 1.基本内置类型：char,unsigned char,signed char int, shortint, longint, float,double,long double 2.自定义类型： C++指针类型指针类型的变量，其值为另一个变量的地址 int *ip; //一个整形的指针 float *fp; //一个浮点型的指针 int *mp; //一个整形指针的指针 int i = 42;int是整数类型，i是变量，42是值 int p = &i;int是指针类型（指向整数）， p是变量，&amp;i返回一个地址用作值"}]